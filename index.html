<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Alison Bartsch</title>

    <meta name="author" content="Alison Bartsch">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="shortcut icon" href="alison_images/blue_arm.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:40px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Alison Bartsch
                </p>
                <p class="nameparagraph"> I'm a PhD candidate in the Mechanical Engineering department at Carnegie Mellon University advised by Prof. Amir Barati Farimani. I am interested in using machine learning for complex and dexterous robotic tasks, such as deformable object manipulation, with the goal of creating robotic systems that are more generalizable and robust to uncertainty.</a>
                  <p class="nameparagraph">Outside of my research, I have a passion for baking, collecting/propagating rare tropical plants, and playing/watching soccer.</a>
                </p>
                <p style="text-align:center">
                  <a href="mailto:abartsch899@gmail.com">Email</a> &nbsp;|&nbsp;
                  <a href="alison_data/abartsch_resume.pdf">CV</a> &nbsp;|&nbsp;
                  <a href="https://scholar.google.com/citations?user=iq2uLQQAAAAJ&hl=en">Scholar</a> &nbsp;|&nbsp;
                  <a href="https://github.com/alison-bartsch/">Github</a> 
                  <!-- <a href="https://www.linkedin.com/in/alison-bartsch/">LinkedIn</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="alison_images/newprofile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="alison_images/newprofile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <!-- <strong>Alison Bartsch</strong> -->

            <!-- <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/bd_promo.jpg" alt="man" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2209.09329">
                  <span class="papertitle">MAN: Multi-Action Networks Learning</span>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/wang-keqin-1b284618a">Keqin Wang</a>, <strong>Alison Bartsch</strong>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>arXiv</em>, 2022
                <p>.</p>
              </td>
            </tr> -->

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/gifs/hierarchical.gif" alt="hierarchical" width="160" height="110">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2412.01765">
                  <span class="papertitle">Planning and Reasoning with 3D Deformable Objects for Hierarchical
                    Text-to-3D Robotic Shaping</span>
                </a>
                <br>
                <strong>Alison Bartsch</strong>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>preprint</em>, 2024
                <br>
                <a href="https://sites.google.com/andrew.cmu.edu/hierarchicalsculpting">project page</a>
                <br>
                <p>A text-to-3D real-world robotic shaping framework. Following a coarse-to-fine approach in which a planner generates a sequence of discrete dough chunks to place into the scene, and a learned refinement model selects refinement actions.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/gifs/llmcraft.gif" alt="llmcraft" width="160" height="110">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2406.08648">
                  <span class="papertitle">LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large
                    Language Models</span>
                </a>
                <br>
                <strong>Alison Bartsch</strong>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>preprint</em>, 2024
                <br>
                <a href="https://sites.google.com/andrew.cmu.edu/llmcraft/home">project page</a>
                <br>
                <p>Leveraging the world knowledge of Large Language Models to generate coherent action sequences for 3D deformable object shaping.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/gifs/sculptdiff.gif" alt="sculptdiff" width="160" height="130">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/10803054">
                  <span class="papertitle">SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal
                    Conditioned Diffusion Policy</span>
                </a>
                <br>
                <strong>Alison Bartsch</strong>, <a href="https://www.cararvind.com/">Arvind Car</a>, <a href="https://charlotteavra.com/">Charlotte Avra</a>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>IEEE International Conference on Intelligent Robots and Systems (IROS)</em>, 2024
                <br>
                <a href="https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home">project page</a>
                <br>
                <p>Goal-conditioned 3D diffusion policy with PointBERT point cloud embeddings for imitation learning for the autonomous clay sculpting task.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/sculptbot.png" alt="sculptbot" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610899">
                  <span class="papertitle">SculptBot: Pre-Trained Models for 3D Deformable Object Manipulation</span>
                </a>
                <br>
                <strong>Alison Bartsch</strong>, <a href="https://charlotteavra.com/">Charlotte Avra</a>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2024
                <br>
                <a href="https://sites.google.com/andrew.cmu.edu/sculptbot">project page</a>
                <br>
                <p>PointBERT model pre-trained on the ShapeNet dataset provides a quality latent embedding of 3D geometrical features of clay relevant to predicting the deformable dynamics given a grasp action.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/plato.png" alt="plato" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2409.11580">
                  <span class="papertitle">PLATO: Planning with LLMs and Affordances for Tool Manipulation</span>
                </a>
                <br>
                <a href="https://www.cararvind.com/">Arvind Car</a>, Sai Sravan Yarlagadda, <strong>Alison Bartsch</strong>, <a href="https://scholar.google.com/citations?user=Bxt7N18AAAAJ&hl=en">Abraham George</a>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>preprint</em>, 2024
                <br>
                <a href="https://sites.google.com/andrew.cmu.edu/plato">project page</a>
                <br>
                <p>Incorporating affordance predictions into an LLM planner pipeline for long-horizon tool use tasks.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/viscosity.png" alt="viscosity" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.sciencedirect.com/science/article/pii/S0952197624007619">
                  <span class="papertitle">Fluid viscosity prediction leveraging computer vision and robot interaction</span>
                </a>
                <br>
                Jong Hoon Park, Gauri Pramod Dalwankar, <strong>Alison Bartsch</strong>, <a href="https://scholar.google.com/citations?user=Bxt7N18AAAAJ&hl=en">Abraham George</a>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>Engineering Applications of Artificial Intelligence</em>, 2024
                <p>Interactive perception framework for vision-based fluid viscosity prediction.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/pour.png" alt="pour" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2309.08892">
                  <span class="papertitle">Pour me a drink: Robotic Precision Pouring Carbonated Beverages into
                    Transparent Containers</span>
                </a>
                <br>
                <a href="https://www.feiyazhu.me/">Feiya Zhu</a>, Shuo Hu, Letian Leng, <strong>Alison Bartsch</strong>, <a href="https://scholar.google.com/citations?user=Bxt7N18AAAAJ&hl=en">Abraham George</a>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>preprint</em>, 2023
                <p>Vision-based autonomous pouring pipeline for transparent container detection, liquid segmentation, and foam formation prediction.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/robochop.png" alt="robochop" width="160" height="100">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2307.13159">
                  <span class="papertitle">RoboChop: Autonomous Framework for Fruit and Vegetable Chopping
                    Leveraging Foundational Models</span>
                </a>
                <br>
                Atharva Dikshit*, <strong>Alison Bartsch*</strong>,<a href="https://scholar.google.com/citations?user=Bxt7N18AAAAJ&hl=en">Abraham George</a>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>preprint</em>, 2023
                <p>Leveraging foundational vision models SAM and YOLO for identifying fruits and vegetables in all states of chopped (whole to diced) for a fully autonomous chopping pipeline.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/min_human.png" alt="min_human" width="160" height="140">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161119">
                  <span class="papertitle">Minimizing Human Assistance: Augmenting a Single Demonstration for
                    Deep Reinforcement Learning</span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=Bxt7N18AAAAJ&hl=en">Abraham George</a>, <strong>Alison Bartsch</strong>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023
                <p>An augmentation strategy that maintains the advantages of seeding RL with human demonstrations, while only requiring a single human demonstration.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/open_vr.png" alt="open_vr" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2305.09765">
                  <span class="papertitle">OpenVR: Teleoperation for Manipulation</span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=Bxt7N18AAAAJ&hl=en">Abraham George</a>, <strong>Alison Bartsch</strong>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>SoftwareX</em>, 2023
                <p>An open-source oculus-based teleoperation system for the Franka manipulator.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="alison_images/man.png" alt="man" width="160" height="140">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2209.09329">
                  <span class="papertitle">MAN: Multi-Action Networks Learning</span>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/wang-keqin-1b284618a">Keqin Wang</a>, <strong>Alison Bartsch</strong>, <a href="https://www.meche.engineering.cmu.edu/directory/bios/barati-farimani-amir.html">Amir Barati Farimani</a>
                <br>
                <em>preprint</em>, 2022
                <p>Factorizing large N-dimensional action spaces into N 1-dimensional action spaces speeds up and improves policy learning for Deep Q-Learning.</p>
              </td>
            </tr>
    

    <!-- <tr onmouseout="nerfcasting_stop()" onmouseover="nerfcasting_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nerfcasting_image'><video  width=100% muted autoplay loop>
          <source src="images/nerfcasting.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nerfcasting.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nerfcasting_start() {
            document.getElementById('nerfcasting_image').style.opacity = "1";
          }

          function nerfcasting_stop() {
            document.getElementById('nerfcasting_image').style.opacity = "0";
          }
          nerfcasting_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://nerf-casting.github.io/">
          <span class="papertitle">NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections</span>
        </a>
        <br>
				
        <a href="https://dorverbin.github.io/">Dor Verbin</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
				<a href="https://phogzone.com/">Peter Hedman</a>,
				<a href="https://benattal.github.io/">Benjamin Attal</a>, <br>
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
				<a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
				<strong>Jonathan T. Barron</strong>
        <br>
        <em>SIGGRAPH Asia</em>, 2024
        <br>
        <a href="https://nerf-casting.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.14871">arXiv</a>
        <p></p>
        <p>
        Carefully casting reflection rays lets us synthesize photorealistic specularities in real-world scenes.
        </p>
      </td>
    </tr> -->


    <!-- <tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bog_image'><video  width=100% muted autoplay loop>
          <source src="images/bog.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/bog.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function bog_start() {
            document.getElementById('bog_image').style.opacity = "1";
          }

          function bog_stop() {
            document.getElementById('bog_image').style.opacity = "0";
          }
          bog_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://creiser.github.io/binary_opacity_grid/">
          <span class="papertitle">Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis
</span>
        </a>
        <br>
				<a href="https://creiser.github.io/">Christian Reiser</a>,
				<a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
				<a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
				<a href="https://dorverbin.github.io/">Dor Verbin</a>,
				<a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
				<a href="https://bmild.github.io/">Ben Mildenhall</a>,
				<strong>Jonathan T. Barron</strong>,
				<a href="https://phogzone.com/">Peter Hedman</a>*,
				<a href="https://www.cvlibs.net/">Andreas Geiger</a>*		
        <br>
        <em>SIGGRAPH</em>, 2024
        <br>
        <a href="https://creiser.github.io/binary_opacity_grid/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=2TPUmGRg8bM">video</a>
        /
        <a href="https://arxiv.org/abs/2402.12377">arXiv</a>
        <p></p>
        <p>
        Applying anti-aliasing to a discrete opacity grid lets you render a hard representation into a soft image, and this enables highly-detailed mesh recovery.
        </p>
      </td>
    </tr> -->

          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Education</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="0"><tbody>
            
            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center"> -->
                <td style="padding:10px;width:0%;vertical-align:middle">
                </td>
                <!-- reduce the sp -->
              <td width="75%" valign="center">
                <strong>BS Aeronautics & Astronautics @ Stanford University.</strong> 2017-2021</a>
                <br>
                <strong>PhD Mechanical Engineering @ Carnegie Mellon University.</strong> 2021-Present</a>
                <br>
              </td>
            </tr>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Awards</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="0"><tbody>
            
            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center"> -->
                <td style="padding:10px;width:0%;vertical-align:middle">
                </td>
                <!-- reduce the sp -->
              <td width="75%" valign="center">
                <strong>CMU Mechanical Engineering Shaw Fellowship, 2024.</strong> In recognition of academic excellence.</a>
                <br>
                <strong>CMU Engineering Dowd Research Fellowship, 2023.</strong> In recognition of cutting-edge research.</a>
                <br>
              </td>
            </tr>

            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This template is taken from Jon Barron's website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
